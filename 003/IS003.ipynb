{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19a8f6d7-311c-4ea1-ba43-e62381a23fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "44501427-8b34-4ed4-8ff7-708676dca91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'Выкачка_очищенная'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cee99f5a-67fb-41c1-a0d4-89d16f58cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index():\n",
    "    term_documents_dict = defaultdict(list)\n",
    "    idx = 0\n",
    "    for root, dirs, files in os.walk(DIR):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt') and file.lower().startswith('lemmas'):\n",
    "                idx += 1\n",
    "                path_file = os.path.join(root, file)\n",
    "                with open(path_file, encoding=\"utf=8\") as f:\n",
    "                    lemmas = list(map(lambda x: x.split(':')[0], f.readlines()))\n",
    "                for lemma in lemmas:\n",
    "                    term_documents_dict[lemma].append(idx)\n",
    "    return term_documents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c692bed-18a4-4b15-90e6-395f10d8ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = set(range(100))\n",
    "inverted_index = get_inverted_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "303f8546-437f-4b1b-b35a-3b65ce114968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    tok = RegexpTokenizer(r'[A-Za-z&(\\|)~\\)\\(]+')\n",
    "    clean = tok.tokenize(s)\n",
    "    clean = [w.lower() for w in clean if w != '']\n",
    "    return list(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a709c9bc-d0db-4ae5-ab4d-4c77896cf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for t in tokens:\n",
    "        if re.match(r'[A-Za-z]', t):\n",
    "            l = lem.lemmatize(t)\n",
    "            lemmas.append(l)\n",
    "        else:\n",
    "            lemmas.append(token)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59c27001-8f9c-4f7c-ba82-a1f559fab426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priority(oper):\n",
    "    if oper == '&':\n",
    "        return 2\n",
    "    elif oper == '|':\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8dc53c81-2ddd-45d2-a043-c6aa076ac425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notaion(operands):\n",
    "    result = []\n",
    "    stack = []\n",
    "    for operand in operands:\n",
    "        if operand not in ['&', '|']:\n",
    "            result.append(operand)\n",
    "        else:\n",
    "            last = None if len(stack) == 0 else stack[-1]\n",
    "            while priority(last) >= priority(operand):\n",
    "                result.append(stack.pop())\n",
    "                last = None if not stack else stack[-1]\n",
    "            stack.append(operand)\n",
    "    for el in reversed(stack):\n",
    "        result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91289312-4513-4910-9838-f2c03b94d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word):\n",
    "    if word[0] == '~':\n",
    "        try:\n",
    "            indices = set(inverted_index[word[1:]])\n",
    "            return DOCS - indices\n",
    "        except KeyError:\n",
    "            return set()\n",
    "    else:\n",
    "        try:\n",
    "            index = inverted_index[word]\n",
    "            return set(index)\n",
    "        except KeyError:\n",
    "            return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41915dc8-3e1d-4ca7-84f5-cef09da026ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokens):\n",
    "    stack = []\n",
    "    for token in tokens:\n",
    "        if token in ['&', '|']:\n",
    "            arg2, arg1 = stack.pop(), stack.pop()\n",
    "            if token == '&':\n",
    "                result = arg1 & arg2\n",
    "            else:\n",
    "                result = arg1 | arg2\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            stack.append(get_index(token))\n",
    "    return stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74125706-cc7f-440a-b14f-0786cb68bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_query(query):\n",
    "    negations_indices = []\n",
    "    tokenized_query = []\n",
    "\n",
    "    for (index, word) in enumerate(query.split(' ')):\n",
    "        if word == '&' or word == '|':\n",
    "            tokenized_query.append(word)\n",
    "        else:\n",
    "            if word[0] == '~':\n",
    "                tokenized_word = lemmatize(tokenize(word[1:]))[0]\n",
    "                tokenized_query.append('~' + tokenized_word)\n",
    "            else:\n",
    "                tokenized_word = lemmatize(tokenize(word))[0]\n",
    "                tokenized_query.append(tokenized_word)\n",
    "\n",
    "    return tokenized_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e859558-89f4-46da-aea4-b42fbe1c0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    tokenized_query = tokenize_query(query)\n",
    "    print(tokenized_query)\n",
    "    converted_query = get_notaion(tokenized_query)\n",
    "    print(converted_query)\n",
    "    result = evaluate(converted_query)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae98b69a-a637-4c1e-be89-07cd70b7db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_dict = get_inverted_index()\n",
    "with open('inverted_index.txt', 'w', encoding='utf-8') as f:\n",
    "    for k, v in td_dict.items():\n",
    "        f.write(k + ' ' + ' '.join(map(str, v)) + '\\n')\n",
    "count_inverted_word = []\n",
    "for k, v in td_dict.items():\n",
    "    count_inverted_word.append({\"count\": len(v), \"inverted_array\": v, \"word\": k})\n",
    "with open('inverted_index_2.txt', 'w', encoding='utf-8') as f:\n",
    "    for ciw in count_inverted_word:\n",
    "        f.write(str(ciw) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "011e0290-13cf-4e10-9abc-7bfa806b8f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', '&', 'bar']\n",
      "['time', 'bar', '&']\n",
      "{1}\n"
     ]
    }
   ],
   "source": [
    "query = \"time & bar\" ## <---Запрос вводить сюда\n",
    "search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c6113-897f-4f44-abdb-f08121ebc71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
